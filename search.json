[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Working on the Sanger HPC cluster using LSF",
    "section": "",
    "text": "Overview\nKnowing how to work on a High Performance Computing (HPC) system is an essential skill for applications such as bioinformatics, big-data analysis, image processing, machine learning, parallelising tasks, and other high-throughput applications. In this module we will cover the basics of High Performance Computing, what it is and how you can use it in practice. This is a hands-on workshop, which should be accessible to researchers from a range of backgrounds and offering several opportunities to practice the skills we learn along the way.\nThe content is specifically tailored for the Sanger HPC server (aka the “FARM”), and by the end of the course you should be well equipped to start running your analysis on your local computing infrastructure.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Working on the Sanger HPC cluster using LSF",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\nDescribe how a HPC cluster is typically organised and how it differs from a regular computer.\nRecognise the tasks that a HPC cluster is suitable for.\nAccess and work on a HPC server.\nSubmit and manage jobs running on a HPC.\nParalelise similar tasks at scale.\nAccess, install and manage software on a HPC.\n\n\n\n\nTarget Audience\nThis course is aimed at students and researchers of any background. We assume no prior knowledge of what a HPC is or how to use it.\nMuch of the content is specific for the servers at the Wellcome Sanger Institute, although the concepts covered would apply to other clusters using the same job scheduler (LSF).\n\n\nPrerequisites\nWe assume a solid knowledge of the Unix command line. If you don’t feel comfortable with the command line, please attend our accompanying Introduction to the Unix Command Line course.\nNamely, we expect you to be familiar with the following:\n\nNavigate the filesystem: pwd (where am I?), ls (what’s in here?), cd (how do I get there?)\nInvestigate file content using utilities such as: head/tail, less, cat/zcat, grep\nUsing “flags” to modify a program’s behaviour, for example: ls -l\nRedirect output with &gt;, for example: echo \"Hello world\" &gt; some_file.txt\nUse the pipe | to chain several commands together, for example ls | wc -l\nExecute shell scripts with bash some_script.sh",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#citation-authors",
    "href": "index.html#citation-authors",
    "title": "Working on the Sanger HPC cluster using LSF",
    "section": "Citation & Authors",
    "text": "Citation & Authors\nPlease cite these materials if:\n\nYou adapted or used any of them in your own teaching.\nThese materials were useful for your research work. For example, you can cite us in the methods section of your paper: “We carried our analyses based on the recommendations in YourReferenceHere”.\n\n\nYou can cite these materials as:\n\nTavares, H., Pacyna, C., Kalmár, L., Clapham, P., Constable, J., Holland, D. (2024). Working on the Sanger HPC cluster using LSF. https://cambiotraining.github.io/hpc-intro-sanger/\n\nOr in BibTeX format:\n@misc{YourReferenceHere,\n  author = {Tavares, Hugo and Pacyna, Chloe and Kalmár, Lajos and Clapham, Peter and Constable, John and Holland, Dave},\n  month = {9},\n  title = {Working on the Sanger HPC cluster using LSF},\n  url = {https://cambiotraining.github.io/hpc-intro-sanger/},\n  year = {2024}\n}\nAbout the authors:\nHugo Tavares  \nAffiliation: Cambridge Centre for Research Informatics Training Roles: writing - original draft; conceptualisation; software\n\nChloe Pacyna  \nAffiliation: Wellcome Sanger Institute Roles: writing - original content; conceptualisation; software\n\nLajos Kalmár  \nAffiliation: MRC Toxicology Unit, University of Cambridge Roles: conceptualisation; software\n\nPeter Clapham  \nAffiliation: Informatics Support Group, Wellcome Sanger Institute Roles: expert review; software\n\nJohn Constable  \nAffiliation: Informatics Support Group, Wellcome Sanger Institute Roles: expert review; software\n\nDave Holland  \nAffiliation: Informatics Support Group, Wellcome Sanger Institute Roles: expert review; software",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Working on the Sanger HPC cluster using LSF",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nThanks to the Informatics Support Group (High Performance Computing) at the Sanger, for providing guest accounts to our trainers, providing technical support during the workshops and for valuable feedback on the course content.\nThanks to Qi Wang (Department of Plant Sciences, University of Cambridge) for constructive feedback and ideas in the early iterations of this course.\nThanks to @Alylaxy for his pull requests to the repo (#34).\nThanks to the HPC Carpentry community for developing similar content.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Overview</span>"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Data & Setup",
    "section": "",
    "text": "Software\nThere are three recommended pieces of software needed to work with the HPC:\nThis document gives instructions on how to install or access these on different operating systems.",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#software",
    "href": "setup.html#software",
    "title": "Data & Setup",
    "section": "",
    "text": "a terminal\na file transfer software\na text editor with the ability to edit files on a remote server\n\n\n\nUnix terminal\n\nWindowsmacOSLinux\n\n\nIf you are comfortable with installing software on your computer, we highly recommend installing the Windows Subsystem for Linux (WSL2), which provides native Linux functionality from within Windows.\nAlternatively, you can install MobaXterm, which provides a Unix-like terminal on Windows.\nWe provide instructions for both.\n\nMobaXtermWSL\n\n\n\nGo the the MobaXterm download page.\nDownload the “Portable edition” (blue button).\n\nUnzip the downloaded file and copy the folder to a convenient location, such as your Desktop.\nYou can directly run the program (without need for installation) from the executable in this folder.\n\n\nYou can access your Windows files from within MobaXterm. Your C:\\ drive is located in /drives/C/ (equally, other drives will be available based on their letter). For example, your documents will be located in: /drives/C/Users/&lt;WINDOWS USERNAME&gt;/Documents/. By default, MobaXterm creates shortcuts for your Windows Documents and Desktop.\nIt may be convenient to set shortcuts to other commonly-used directories, which you can do using symbolic links. For example, to create a shortcut to Downloads: ln -s /drives/C/Users/&lt;WINDOWS USERNAME&gt;/Downloads/ ~/Downloads\n\n\nThere are detailed instructions on how to install WSL on the Microsoft documentation page. But briefly:\n\nClick the Windows key and search for Windows PowerShell, right-click on the app and choose Run as administrator.\nAnswer “Yes” when it asks if you want the App to make changes on your computer.\nA terminal will open; run the command: wsl --install.\n\nThis should start installing “ubuntu”.\nIt may ask for you to restart your computer.\n\nAfter restart, click the Windows key and search for Ubuntu, click on the App and it should open a new terminal.\nFollow the instructions to create a username and password (you can use the same username and password that you have on Windows, or a different one - it’s your choice).\nYou should now have access to a Ubuntu Linux terminal. This (mostly) behaves like a regular Ubuntu terminal, and you can install apps using the sudo apt install command as usual.\n\nAfter WSL is installed, it is useful to create shortcuts to your files on Windows. Your C:\\ drive is located in /mnt/c/ (equally, other drives will be available based on their letter). For example, your desktop will be located in: /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Desktop/. It may be convenient to set shortcuts to commonly-used directories, which you can do using symbolic links, for example:\n\nDocuments: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Documents/ ~/Documents\n\nIf you use OneDrive to save your documents, use: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/OneDrive/Documents/ ~/Documents\n\nDesktop: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Desktop/ ~/Desktop\nDownloads: ln -s /mnt/c/Users/&lt;WINDOWS USERNAME&gt;/Downloads/ ~/Downloads\n\n\n\n\n\n\nMac OS already has a terminal available.\nPress ⌘ + space to open spotlight search and type “terminal”.\nOptionally, if you would like a terminal with more modern features, we recommend installing iTerm2.\n\n\nLinux distributions already have a terminal available.\nOn Ubuntu you can press Ctrl + Alt + T to open it.\n\n\n\n\n\nFilezilla\n\nWindowsmacOSLinux\n\n\n\nGo to the Filezilla Download page and download the file FileZilla_3.65.0_win64-setup.exe (the latest version might be slightly different). Double-click the downloaded file to install the software, accepting all the default options.\nAfter completing the installation, go to your Windows Menu, search for “Filezilla” and launch the application, to test that it was installed successfully.\n\n\n\n\nGo to the Filezilla Download page and download either the macOS (Intel) (for older processors) or macOS (Apple Silicon) (for newer M* processors) installers.\nGo to the Downloads folder and double-click the file you just downloaded to extract the application. Drag-and-drop the “Filezilla” file into your “Applications” folder.\nYou can now open the installed application to check that it was installed successfully (the first time you launch the application you will get a warning that this is an application downloaded from the internet - you can go ahead and click “Open”).\n\n\n\n\nFilezilla often comes pre-installed in major Linux distributions such as Ubuntu. Search your applications to check that it is installed already.\nIf it is not, open a terminal and run:\n\nUbuntu: sudo apt-get update && sudo apt-get install filezilla\nCentOS: sudo yum -y install epel-release && sudo yum -y install filezilla\n\n\n\n\n\n\n\nVisual Studio Code (optional)\n\nWindowsmacOSLinux\n\n\n\nGo to the Visual Studio Code download page and download the installer for your operating system. Double-click the downloaded file to install the software, accepting all the default options.\nAfter completing the installation, go to your Windows Menu, search for “Visual Studio Code” and launch the application.\nGo to “File &gt; Preferences &gt; Settings”, then select “Text Editor &gt; Files” on the drop-down menu on the left. Scroll down to the section named “EOL” and choose “\\n” (this will ensure that the files you edit on Windows are compatible with the Linux operating system).\nContinue by following the instructions “Configuring Visual Studio Code”.\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for Mac.\nGo to the Downloads folder and double-click the file you just downloaded to extract the application. Drag-and-drop the “Visual Studio Code” file to your “Applications” folder.\nYou can now open the installed application to check that it was installed successfully (the first time you launch the application you will get a warning that this is an application downloaded from the internet - you can go ahead and click “Open”).\nContinue by following the instructions “Configuring Visual Studio Code”.\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for your Linux distribution. Install the package using your system’s installer.\nContinue by following the instructions in “Configuring Visual Studio Code”.\n\n\n\n\n\nConfiguring Visual Studio Code\nWe will use an extension called “Remote-SSH”. To install the extension (see Figure 2.1):\n\nClick the “Extensions” button on the side bar (or use Ctrl + Shift + X).\nIn the search box type “remote ssh” and choose the “Remote - SSH” extension.\nClick the “Install” button in the window that opens.\nRestart VS Code.\nGo to File → Preferences → Settings\nIn the search box type “Remote SSH: Show Login Terminal”\nTick the option “Always reveal the SSH login terminal”\n\n\n\n\n\n\n\nFigure 2.1: Installing Remote-SSH extension in VS Code",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "setup.html#data",
    "href": "setup.html#data",
    "title": "Data & Setup",
    "section": "Data",
    "text": "Data\nPlease download the workshop data (zip file) and save it to your desktop (do not unzip the file).\n  Download",
    "crumbs": [
      "Slides",
      "Welcome",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data & Setup</span>"
    ]
  },
  {
    "objectID": "materials/01-intro.html",
    "href": "materials/01-intro.html",
    "title": "3  HPC Introduction",
    "section": "",
    "text": "3.1 What is a HPC and what are its uses?\nHPC stands for high-performance computing and usually refers to several computers connected together in a network (forming a HPC cluster). Each of these computers is referred to as a node in the network.\nThe main usage of HPC clusters is to run resource-intensive and/or parallel tasks. For example: running thousands of simulations, each one taking several hours; assembling a genome from sequencing data, which requires computations on large volumes of data in memory. These tasks would be extremely challenging to complete on a regular computer. However, they are just the kind of task that a HPC would excel at.\nWhen working on a HPC it is important to understand what kinds of resources are available to us. These are the main resources we need to consider:\nUsually, HPC servers are available to members of large institutions (such as a Universities or research institutes) or sometimes from cloud providers. This means that:\nSo, at any one time, across all the users, there might be many thousands of processes running on the HPC! There has to be a way to manage all this workload, and this is why HPC clusters are typically organised somewhat differently from what we might be used to when we work on our own computers. Figure 1 shows a schematic of a HPC, and we go into its details in the following sections.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Introduction</span>"
    ]
  },
  {
    "objectID": "materials/01-intro.html#what-is-a-hpc-and-what-are-its-uses",
    "href": "materials/01-intro.html#what-is-a-hpc-and-what-are-its-uses",
    "title": "3  HPC Introduction",
    "section": "",
    "text": "Terminology Alert!\n\n\n\nThe terms HPC, cluster, and farm are often used interchangeably to mean the same thing.\n\n\n\n\nCPU (central processing units) is the “brain” of the computer, performing a wide range of operations and calculations. CPUs can have several “cores”, which means they can run tasks in parallel, increasing the throughput of calculations per second. A typical personal computer may have a CPU with 4-8 cores. A single compute node on the HPC may have 32-48 cores (and often these are faster than the CPU on our computers).\nRAM (random access memory) is a quick access storage where data is temporarily held while being processed by the CPU. A typical personal computer may have 8-32Gb of RAM. A single compute nodes on a HPC may often have &gt;100Gb RAM.\nGPUs (graphical processing units) are similar to CPUs, but are more specialised in the type of operations they can do. While less flexible than CPUs, each GPU can do thousands of calculations in parallel. This makes them extremely well suited for graphical tasks, but also more generally for matrix computations and so are often used in machine learning applications.\n\n\n\nThere are many users, who may simultaneously be using the HPC.\nEach user may want to run several jobs concurrently.\nOften large volumes of data are being processed and there is a need for high-performance storage (allowing fast read-writting of files).\n\n\n\n\n\nOrganisation of a typical HPC.\n\n\n\n3.1.1 Nodes\nThere are two types of nodes on a cluster (Figure 1):\n\nlogin nodes (also known as head or submit nodes).\ncompute nodes (also known as worker nodes).\n\nThe login nodes are the computers that the user connects to and from where they interact with the cluster. Depending on the size of the cluster, there is often only one login node, but larger clusters may have several of them. Login nodes are used to interact with the filesystem (move around the directories), download and move files, edit and/or view text files and doing other small routine tasks.\nThe compute nodes are the machines that will actually do the hard work of running jobs. These are often high-spec computers with many CPUs and high RAM (or powerful GPU cards), suitable for computationally demanding tasks. Often, there are several “flavours” of compute nodes on the same cluster. For example some compute nodes may have fewer CPUs but higher memory (suitable for memory-intensive tasks), while others may have the opposite (suitable for highly-parallelisable tasks).\nUsers do not have direct access to the compute nodes and instead submitting jobs via a job scheduler.\n\n\n3.1.2 Job Scheduler\nA job scheduler is a software used to submit commands to be run on the compute nodes (orange box in Figure 1). This is needed because there may often be thousands of processes that all the users of the HPC want to run at any one time. The job scheduler’s role is to manage all these jobs, so you don’t have to worry about it.\nWe will cover the details of how to use a job scheduler in “Using the LSF Job Scheduler”. For now, it is enough to know that, using the job scheduler, the user can request specific resources to run their job (e.g. number of cores, RAM, how much time we want to reserve the compute node to run our job, etc.). The job scheduler software then takes care of considering all the jobs being submitted by all the users and putting them in a queue until there are compute nodes available to run the job with the requested resources.\n\n\n\nAn analogy of the role of a job scheduler. You can think of a job scheduler as a porter in a restaurant, who checks the groups of people in the queue and assigns them a seat depending on the size of the group and how long they might stay for dinner.\n\n\nIn terms of parallelising calculations, there are two ways to think about it, and which one we use depends on the specific application. Some software packages have been developed to internally parallelise their calculations (or you may write your own script that uses a parallel library). These are very commonly used in bioinformatic applications, for example. In this case we may want to submit a single job, requesting several CPU cores for it.\nIn other cases, we may have a program that does not parallelise its calculations, but we want to run many iterations of it. A typical example is when we want to run simulations: each simulation only uses a single core, but we want to run thousands of them. In this case we would want to submit each simulation as a separate job, but only request a single CPU core for each job.\nFinally, we may have a case where both of these are true. For example, we want to process several data files, where each data file can be processed using tools that parallelise their calculations. In this case we would want to submit several jobs, requesting several CPU cores for each.\n\n\n\n\n\n\nJob Schedulers\n\n\n\nThere are many job scheduler programs available. In this course we will cover one called LSF because the Sanger Institute uses it, but other common ones include SLURM, PBS, HT Condor, among others.\n\n\n\n\n3.1.3 Filesystem\nThe filesystem on a HPC cluster often consists of storage partitions that are shared across all the nodes, including both the login and compute nodes (green box in Figure 1). This means that data can be accessed from all the computers that compose the HPC cluster.\nAlthough the filesystem organisation may differ depending on the institution, typical HPC servers often have two types of storage:\n\nThe user’s home directory (e.g. /home/user) is the default directory that one lands on when logging in to the HPC. This is often quite small and possibly backed up. The home directory can be used for storing things like configuration files or locally installed software.\nA scratch space (e.g. /scratch/user), which is high-performance, large-scale storage. This type of storage may be private to the user or shared with a group. It is usually not backed up, so the user needs to ensure that important data are stored elsewhere. This is the main partition were data is processed from.\n\n\nSanger Filesystem\nAt the Sanger Institute, the HPC is called the “farm” and is organised into 3 storage partitions:\n\nAn NFS directory with long-term, backed-up storage for data, located at /nfs. In addition to housing your home directory (at /nfs/users/nfs_[first_initial_of_username]/[sanger_username]), this contains iRODS directories where sequencing data from pipelines are kept, programme-specific file storage systems like cancer_ref for CASM, and team-specific storage. NFS files are intended to be ‘read only’ as computation should not be done directly from this folder; you’re advised to move files over to scratch for heavy input/output work. This moving step is called “staging” at the Sanger, and there is an internal module to help with this process.\nA system of scratch directories stored in a system called lustre housed in /lustre. Lustre has a lot of storage but is not backed up. There are a series of scratch folders within lustre, each with nested subfolders for Sanger programmes (e.g. casm), groups (e.g. team154pc), and users (e.g. cp19). Permissions to each folder are managed by the Service Desk — to join your group’s scratch folder you must first identify which directories you need access to (ask a labmate) and email the Service Desk (service@sanger.ac.uk). For example, /lustre/scratchxxx/team154pc/cp19.\nWarehouse is another long-term storage system predominantly used by the Service Desk to arrange group permissions. You most likely won’t need this at all in your day-to-day work (which should mainly be in lustre).\n\n\n\n\nFarm overview.\n\n\n\n\nExample workflow for Sanger data analysis\nThe Sanger has pipelined both wet lab sequencing steps and initial data analysis (i.e. aligning and variant calling). You have to request these using programme-specific workflows, but the actual computational steps are done by the pipelines teams. The resultant raw data and initial data analysis outfiles are stored within NFS, which you can access for further analysis and visualisation.\n\nSubmit a request for sequencing.\n\nThrough this process, your submission will be assigned a project ID number and a sample ID (PD*****).\n\nRequest additional analysis. (optional)\n\nIn addition to standard alignment, some programmes (CASM, CellGen, etc.) offer in-house analysis like variant calling for DNA or HTSeq for RNA. This is requested by either emailing your programme’s service desk or through programme-specific webpages (i.e. Canapps).\n\nLocate raw data or processed data.\n\nAll Sanger data are stored on iRODS, an efficient storage system separate from the Farm. Instructions for accessing these raw or aligned data are here.\nSome programmes may symlink iRODS data to designated NFS directories for easy access of both BAM files and pipelined analysis (variant calling, structural variant calling, etc.).\n\nTransfer data from NFS to Lustre.\n\nWe can only read files within NFS (iRODS or programme-specific directory), so for downstream analysis the files must be moved to your Lustre scratch folder. This moving process is called staging and there is a Sanger module that makes this process easy and efficient.\n\nAnalyse data.\n\nSubmit a job to the LSF scheduler using bsub.\n\nTransfer results from Farm to local computer.\n\nUse a FTP tool to move desired results to your local machine.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Introduction</span>"
    ]
  },
  {
    "objectID": "materials/01-intro.html#getting-help",
    "href": "materials/01-intro.html#getting-help",
    "title": "3  HPC Introduction",
    "section": "3.2 Getting Help",
    "text": "3.2 Getting Help\nIn most cases there will be a HPC administrator (or team), who you can reach out for help if you need to obtain more information about how your HPC is organised. At the Sanger, you can contact the Service Desk at service@sanger.ac.uk with farm-related queries.\nSome of the questions you may want to ask when you start using a new HPC are:\n\nwhat kind of compute nodes are available?\nwhat storage do I have access to, and how much?\nwhat job scheduler software is used, and can you give me an example submission script to get started?\nwill I be charged for the use of the HPC?\n\nAlso, it is often the case that the HPC needs some maintenance service, and you should be informed that this is happening (e.g. by a mailing list). At the Sanger, this mailing list is farm-users@sanger.ac.uk, to which you’ll be automatically added once you complete the Farm Induction course and gain access to the farm.\nSometimes things stop working or break, and there may be some time when your HPC is not available while work is being done on it. You’ll be kept in the loop on these outages via the farm users mailing list.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Introduction</span>"
    ]
  },
  {
    "objectID": "materials/01-intro.html#exercises",
    "href": "materials/01-intro.html#exercises",
    "title": "3  HPC Introduction",
    "section": "3.3 Exercises",
    "text": "3.3 Exercises\n\n\n\n\n\n\nExercise 1\n\n\n\n\n\n\nA PhD student with username ab12 wants to process some sequencing data from Sanger pipelines using a python script developed by a postodoc colleague. They have instructions for how to install the necessary python packages, and also the actual python script to process the data.\nQ1. Which of the following describes the best practice for the student to organise their files/software?\nOption A:\n/lustre/scratch123/ab12/project_name/software/ # python packages\n/lustre/scratch123/ab12/project_name/data/     # sequencing data\n/lustre/scratch123/ab12/project_name/analysis_scripts/  # analysis script\nOption B:\n/nfs/users/nfs_a/ab12/software/               # python packages\n/lustre/scratch123/ab12/project_name/data/    # sequencing data\n/lustre/scratch123/ab12/project_name/analysis_scripts/ # analysis script\nOption C:\n/nfs/users/nfs_c/ab12/project_name/software/   # python packages\n/nfs/users/nfs_c/ab12/project_name/data/       # sequencing data\n/nfs/users/nfs_c/ab12/project_name/analysis_scripts/    # analysis script\nOption D:\n/nfs/users/nfs_c/ab12/project_name/software/                        # python packages\n/nfs/cancer_ref01/nst_links/live/sequencing_project_number12345/    # sequencing data\n/lustre/scratch123/ab12/project_name/analysis_scripts/                       # analysis script\nQ2. A collaborator has send the student some additional metadata files, which were compressed as a zip file. The postdoc told the student they can run unzip sequencing_files.zip to decompress the file. Should they run this command from the login node or submit it as a job to one of the compute nodes?\nQ3. The analysis script used by the student generates new output files. In total, after processing the data, the student ends up with ~300GB of data (raw + processed files). Their group still has 1TB of free space in their group scratch folder, so the student decides to keep the data there until they finish the project. Do you agree with this choice, and why? What factors would you take into consideration in deciding what data to keep and where?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA1.\nDespite the long paths, we can put them into shorthand:\n\n/nfs/users/nfs_c/ab12/ is the user’s home (within NFS)\n/lustre/scratch123/ab12/ is the user’s scratch (within lustre)\n/nfs/cancer_ref01/nst_links/live/ is part of NFS\n\nOption C is definitely discouraged: as /nfs is typically not high-performance and has limited storage, it should not be used for storing/processing data.\nOption D does have the correct path for where the sequencing data are stored from Sanger pipelines (one of the iRODs directories), but it isn’t fully correct either. To do analysis on these data, the student must move files from NFS (read only) to lustre (read, write, execute for high i/o) - this is called “staging”.\nFinally, options A and B only differ in terms of where the software packages are installed. Typically software can be installed in the user’s home directory, avoiding the need to reinstall it multiple times, in case the same software is used in different projects. Therefore, if the student first moves files from NFS (iRODs) to lustre (their high-performance scratch space), then option B is the best practice in this example.\nA2.\nSince compressing/uncompressing files is a fairly routine task and unlikely to require too many resources, it would be OK to run it on the login node. If in doubt, the student could have gained “interactive” access to one of the compute nodes (we will cover this in another section).\nA3.\nLeaving raw data in scratch permanently is probably a bad choice. Since typically “scratch” storage is not backed-up it should not be relied on to store important data.\nIf the student doesn’t have access to enough backed-up space for all the data, they should at least back up the raw data and the scripts used to process it. This way, if there is a problem with “scratch” and some processed files are lost, they can recreate them by re-running the scripts on the raw data. Because raw data straight from Sanger pipelines are deposited into NFS for safe keeping, the student doesn’t need to keep a second copy on lustre. Once their analysis is complete, those “staged” copies in lustre should be deleted.\nIf the student were working with original, raw, non-Sanger pipelines data not deposited directly to scratch, they should to contact the Service Desk about finding a long-term storage plan for it within NFS.\nOther criteria that could be used to decide which data to leave on the HPC, backup or even delete is how long each step of the analysis takes to run, as there may be a significant computational cost associated with re-running heavy data processing steps.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Introduction</span>"
    ]
  },
  {
    "objectID": "materials/01-intro.html#summary",
    "href": "materials/01-intro.html#summary",
    "title": "3  HPC Introduction",
    "section": "3.4 Summary",
    "text": "3.4 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nA HPC consists of several computers connected in a network. These are called nodes:\n\nThe login nodes are the machines that we connect to and from where we interact with the HPC. These should not be used to run resource-intensive tasks.\nThe compute nodes are the high-performance machines on which the actual heavy computations run. Jobs are submitted to the compute nodes through a job scheduler.\n\nThe job scheduler is used to submit scripts to be run on the compute nodes.\n\nThe role of this software is to manage large numbers of jobs being submitted and prioritise them according to their resource needs.\nWe can configure how our jobs are run by requesting the adequate resources (CPUs and RAM memory).\nChoosing resources appropriately helps to get our jobs the right level of priority in the queue.\n\nThe filesystem on a HPC is often split between a small (backed) home directory, and a large and high-performance (non-backed) scratch space.\n\nThe user’s home is used for things like configuration files and local software instalation.\nThe scratch space is used for the data and analysis scripts.\nNot all HPC servers have this filesystem organisation - always check with your local HPC admin.\n\nAt the Sanger there are two types of storage:\n\nNFS: used for the user’s home directories and for iRODs (which contains sequencing data from the Sanger pipelines).\nLustre: used for high-performance compute operations, and the designated storage that should be used for running analysis on the HPC.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>HPC Introduction</span>"
    ]
  },
  {
    "objectID": "materials/02-ssh.html",
    "href": "materials/02-ssh.html",
    "title": "4  Working on the Farm",
    "section": "",
    "text": "4.1 Connecting to the HPC\nAll interactions with the farm happen via the terminal (or command line). To connect to the HPC we use the program ssh. The syntax is:\nOn the Sanger Farm there are two servers:\nTo log onto the Sanger Farm you can use the SSH command:\nThe Sanger service desk has set up your laptop to assume your Sanger ID, so you can use the simplified command to access the Farm:\nThe first time you connect to an HPC, you may receive a message about the ECDSA key fingerprint. By typing yes you’ll add the ‘fingerprint’ of this HPC to your local computer’s saved list of approved hosts.\nAfter running this ssh command and approving any ECDSA key questions, you will be asked for your Sanger password and after typing it you will be logged in to the Farm.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working on the Farm</span>"
    ]
  },
  {
    "objectID": "materials/02-ssh.html#connecting-to-the-hpc",
    "href": "materials/02-ssh.html#connecting-to-the-hpc",
    "title": "4  Working on the Farm",
    "section": "",
    "text": "ssh your-hpc-username@hpc-server-address\n\n\nfarm22 is the main HPC, which you can only access after attending the required Farm and Data Management courses.\ngen22 is a test environment, providing a smaller smaller HPC similar in structure to the larger farm22. This is useful to learn about the Farm and is what we will use in these materials. Everyone with a Sanger ID has access to gen22.\n\n\nssh sanger-username@gen22-login\n\nssh gen22\n\n\n\n\n\nLogin to HPC using the terminal.  1) Use the ssh program to login to gen22/farm22.  2) If prompted, approve ECDSA key fingerprint by typing “yes”.  3) Type your Sanger password. Note that as you type the password, nothing shows on the screen - that’s normal, the terminal is recording your password!  4) You will receive a login message and notice that your terminal will now indicate your Sanger username and the name of the server you connected to.\n\n\n\n4.1.1 Exercise\n\n\n\n\n\n\nExercise 1 - Connecting to the HPC\n\n\n\n\n\n\nQ1. Connect to gen22 using ssh\nQ2. Take some time to explore your home directory to identify what files and folders are in there. Can you identify and navigate through the scratch (Lustre) and NFS directories?\nQ3. Print the path to your home directory.\nQ4. Create a directory called hpc_workshop in your own home directory.\nQ5. Use the commands free -h (available RAM memory) and nproc --all (number of CPU cores available) to check the capabilities of the login node of our HPC. Check how many people are logged in to the HPC login node using the command who.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA1.\nTo login to the HPC we run the following from the terminal:\nssh USERNAME@gen22\n(replace “USERNAME” with your Sanger username)\nA2.\nWe can get a detailed list of the files on our home directory:\nls -l\nFurther, we can explore the NFS directory using:\nls /nfs\nAnd check out the scratch folders available in lustre:\nls -l /lustre\nA3.\nTo find the path of your home directory, move to it and then use the pwd command to print the entire path:\ncd\npwd\nIt should be /nfs/users/nfs_[first_initial_of_username]/[username]\nA4. Once we are in the home directory, we can use mkdir to create our workshop sub-directory:\ncd\nmkdir hpc_workshop\nA5.\nWe run these commands to investigate how much memory and CPUs the login node that we connected to at the moment has. Usually, the login node is not very powerful, and we should be careful not to run any analysis on it.\nTo see how many people are currently on the login node we can combine the who and wc commands:\n# pipe the output of `who` to `wc`\n# the `-l` flag instructs `wc` to count \"lines\" of its input\nwho | wc -l\nYou should notice that several people are using the same login node as you. This is why we should never run resource-intensive applications on the login node of a HPC.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working on the Farm</span>"
    ]
  },
  {
    "objectID": "materials/02-ssh.html#editing-scripts-remotely",
    "href": "materials/02-ssh.html#editing-scripts-remotely",
    "title": "4  Working on the Farm",
    "section": "4.2 Editing Scripts Remotely",
    "text": "4.2 Editing Scripts Remotely\nMost of the work you will be doing on a HPC is editing script files. These may be scripts that you are developing to do a particular analysis or simulation, for example (in Python, R, Julia, etc.). But also - and more relevant for this course - you will be writing shell scripts containing the commands that you want to be executed on the compute nodes.\nThere are several possibilities to edit text files on a remote server. A simple one is to use the program Nano directly from the terminal. This is a simple text editor available on most linux distributions, and what we will use in this course.\nAlthough Nano is readily available and easy to use, it offers limited functionality and is not as user friendly as a full-featured text editor. You can use other more full-featured text editors from the command line such as vim, but it does come with a steeper learning curve. Alternatively, we recommend Visual Studio Code, which is an open-source software with a wide range of functionality and several extensions, including an extension for working on remote servers.\n\n4.2.1 Nano\n\nTo create a file with Nano you can run the command:\nnano test.sh\nThis opens a text editor, where you can type the code that you want to save in the file. Once we’re happy with our code, we can press Ctrl+O to write our data to disk. We’ll be asked what file we want to save this to: press Enter to confirm the filename. Once our file is saved, we can use Ctrl+X to quit the editor and return to the shell.\nWe can check with ls that our new file is there.\n\n\n\nScreenshot of the command line text editor Nano. In this example, we also included !#/bin/bash in the first line of the script. This is called a shebang and is a way to inform that this script uses the program bash to run the script.\n\n\nNote that because we saved our file with .sh extension (the conventional extension used for shell scripts), Nano does some colouring of our commands (this is called syntax highlighting) to make it easier to read the code.\n\n\n4.2.2 Exercise\n\n\n\n\n\n\nExercise 2 - Editing scripts\n\n\n\n\n\n\n\nCreate a new script file called check_hostname.sh. Copy the code shown below into this script and save it.\nFrom the terminal, run the script using bash.\n\n#!/bin/bash\necho \"This job is running on:\"\nhostname\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA1.\nTo create a new script in Nano we use the command:\nnano check_hostname.sh\nThis opens the editor, where we can copy/paste our code. When we are finished we can click Ctrl+X to exit the program, and it will ask if we would like to save the file. We can type “Y” (Yes) followed by Enter to confirm the file name.\nA2.\nWe can run the script from the terminal using:\nbash test.sh\nWhich should print the result (your hostname might vary slightly from this answer):\nThis job is running on:\ngen22-head1\n(the output might be slightly different if you were assigned to a different login node of the HPC)",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working on the Farm</span>"
    ]
  },
  {
    "objectID": "materials/02-ssh.html#summary",
    "href": "materials/02-ssh.html#summary",
    "title": "4  Working on the Farm",
    "section": "4.3 Summary",
    "text": "4.3 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nThe terminal is used to connect and interact with the HPC.\n\nTo connect to the HPC we use ssh username@remote-hostname.\n\nNano is a text editor that is readily available on HPC systems.\n\nTo create or edit an existing file we use the command nano path/to/filename.sh.\nKeyboard shortcuts are used to save the file (Ctrl + O) and to exit (Ctrl + X).",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Working on the Farm</span>"
    ]
  },
  {
    "objectID": "materials/03-files.html",
    "href": "materials/03-files.html",
    "title": "5  File Transfer",
    "section": "",
    "text": "5.1 Moving Files\nThere are several options to move data between your local computer and a remote server. We will cover three possibilities in this section, which vary in their ease of use.\nA quick summary of these tools is given in the table below.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File Transfer</span>"
    ]
  },
  {
    "objectID": "materials/03-files.html#moving-files",
    "href": "materials/03-files.html#moving-files",
    "title": "5  File Transfer",
    "section": "",
    "text": "Filezilla\nSCP\nRsync\n\n\n\n\nInterface\nGUI\nCommand Line\nCommand Line\n\n\nData synchronisation\nyes\nno\nyes\n\n\n\n\n5.1.1 Filezilla (GUI)\nThis program has a graphical interface, for those that prefer it and its use is relatively intuitive.\nTo connect to the remote server (see Figure 3):\n\nFill in the following information on the top panel:\n\n\nHost: gen22\nUsername: your HPC username\nPassword: your HPC password\nPort: 22\n\n\nClick “Quickconnect” and the files on your “home” should appear in a panel on right side.\nNavigate to your desired location by either clicking on the folder browser or typing the directory path in the box “Remote site:”.\nYou can then drag-and-drop files between the left side panel (your local filesystem) and the right side panel (the HPC filesystem), or vice-versa.\n\n\n\n\nExample of a Filezilla session. Arrows in red highlight: the connection panel, on the top; the file browser panels, in the middle; the transfer progress panel on the bottom.\n\n\n\n\n5.1.2 scp (command line)\nThis is a command line tool that can be used to copy files between two servers. One thing to note is that it always transfers all the files in a folder, regardless of whether they have changed or not.\nThe syntax is as follows:\n# copy files from the local computer to the HPC\nscp -r path/to/source_folder &lt;user&gt;@gen22:path/to/target_folder\n\n# copy files from the HPC to a local directory\nscp -r &lt;user&gt;@gen22:path/to/source_folder path/to/target_folder\nThe option -r ensures that all sub-directories are copied (instead of just files, which is the default).\n\n\n5.1.3 rsync (command line)\nThis program is more advanced than scp and has options to synchronise files between two directories in multiple ways. The cost of its flexibility is that it can be a little harder to use.\nThe most common usage is:\n# copy files from the local computer to the HPC\nrsync -auvh --progress path/to/source_folder &lt;user&gt;@gen22:path/to/target_folder\n\n# copy files from the HPC to a local directory\nrsync -auvh --progress &lt;user&gt;@gen22:path/to/source_folder path/to/target_folder\n\nthe options -au ensure that only files that have changed and are newer on the source folder are transferred\nthe options -vh give detailed information about the transfer and human-readable file sizes\nthe option --progress shows the progress of each file being transferred\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen you specify the source directory as path/to/source_folder/ (with / at the end) or path/to/source_folder (without / at the end), rsync will do different things:\n\npath/to/source_folder/ will copy the files within source_folder but not the folder itself\npath/to/source_folder will copy the actual source_folder as well as all the files within it\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTo check what files rsync would transfer but not actually transfer them, add the --dry-run option. This is useful to check that you’ve specified the right source and target directories and options.\n\n\n\n\n5.1.4 Exercise\n\n\n\n\n\n\nExercise 1 - Copying files to the cluster\n\n\n\n\n\n\n\nDownload the data for this course to your computer and place it on your Desktop. (do not unzip the file yet!)\nUse Filezilla, scp or rsync (your choice) to move this file to the directory we created earlier: ~/hpc_workshop/.\nThe file we just downloaded is a compressed file. From the HPC terminal, use unzip to decompress the file.\n\nNote: macOS often unzips downloaded files by default. If that is the case you can skip this step.\n\nConfirm that all the files are present in your folder, the command ls ~/hpc_workshop should return:\nanalysis_scripts  data  job_logs  job_scripts\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nOnce we download the data to our computer, we can transfer it using either of the suggested programs. We show the solution using command-line tools.\nNotice that these commands are run from your local terminal:\n# with scp\nscp -r ~/Desktop/hpc_workshop_files.zip username@gen22:hpc_workshop/\n\n# with rsync\nrsync -avhu ~/Desktop/hpc_workshop_files.zip username@gen22:hpc_workshop/\nIn our case, the Zip file was downloaded to our Desktop, but you can adjust this depending on where the file was downloaded in your case. For example, another common default is a folder named “Downloads”.\nOnce we finish transfering the files, we can go ahead and decompress the data folder. Note, this is now run from the HPC terminal:\n# make sure to be in the correct directory\ncd ~/hpc_workshop/\n\n# decompress the files\nunzip hpc_workshop_files.zip",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File Transfer</span>"
    ]
  },
  {
    "objectID": "materials/03-files.html#summary",
    "href": "materials/03-files.html#summary",
    "title": "5  File Transfer",
    "section": "5.2 Summary",
    "text": "5.2 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nTo transfer files to/from the HPC we can use Filezilla, which offers a user-friendly interface to synchronise files between your local computer and a remote server.\nTransfering files can also be done from the command line, using tools such as scp and rsync (this is the most flexible tool but also more advanced).",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>File Transfer</span>"
    ]
  },
  {
    "objectID": "materials/04-lsf.html",
    "href": "materials/04-lsf.html",
    "title": "6  LSF Scheduler",
    "section": "",
    "text": "6.1 Job Scheduler Overview\nAs we briefly discussed in “Introduction to HPC”, HPC servers usually have a job scheduling software that manages all the jobs that the users submit to be run on the compute nodes. This allows efficient usage of the compute resources (CPUs and RAM), and the user does not have to worry about affecting other people’s jobs.\nThe job scheduler uses an algorithm to prioritise the jobs, weighing aspects such as:\nBased on these, the algorithm will rank each of the jobs in the queue to decide on a “fair” way to prioritise them. Note that this priority dynamically changes all the time, as jobs are submitted or cancelled by the users, and depending on how long they have been in the queue. For example, a job requesting many resources may start with a low priority, but the longer it waits in the queue, the more its priority increases.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LSF Scheduler</span>"
    ]
  },
  {
    "objectID": "materials/04-lsf.html#job-scheduler-overview",
    "href": "materials/04-lsf.html#job-scheduler-overview",
    "title": "6  LSF Scheduler",
    "section": "",
    "text": "how much time did you request to run your job?\nhow many resources (CPUs and RAM) do you need?\nhow many other jobs have you got running at the moment?",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LSF Scheduler</span>"
    ]
  },
  {
    "objectID": "materials/04-lsf.html#submitting-a-job-with-lsf",
    "href": "materials/04-lsf.html#submitting-a-job-with-lsf",
    "title": "6  LSF Scheduler",
    "section": "6.2 Submitting a Job with LSF",
    "text": "6.2 Submitting a Job with LSF\nTo submit a job to LSF, you need to include your code in a shell script. Let’s start with a minimal example in job_scripts/simple_job.sh, which contains the following code:\n#!/bin/bash\n\nsleep 60 # hold for 60 seconds\necho \"This job is running on:\"\nhostname\nWe can run this script from the login node using the bash interpreter (make sure you are in the correct directory first: cd ~/hpc_workshop/):\nbash job_scripts/simple_job.sh\nThis job is running on:\ngen22-head1\nTo submit the job to the scheduler we instead use the bsub command in a very similar way:\nbsub job_scripts/simple_job.sh\nHowever, this throws back an error:\nReturning output by mail is not supported on this cluster.\nPlease use the -o option to write output to disk.\nRequest aborted by esub. Job not submitted.\nOur job, like all LSF jobs, has an output (in this case, the outputs of the echo and hostname commands) and job statistics about what was done on the HPC. Because the Sanger LSF is set up to disallow outputs to be sent to your email by default for security reasons, it is impossible for the job to run without specifying a “standard output” file.\nWe can fix this error using the -o argument:\nbsub -o simple_job.out job_scripts/simple_job.sh\nThis ensures the output of our job is sent to a file, which we called simple_job.out. By default, this file will be located in the directory where you launched the job from.\nHowever, we now get another error:\nSorry no available user group specified for this job. \nPlease resubmit your job with -G groupname or set the $LSB_DEFAULT_USERGROUP environment variable.\nRequest aborted by esub. Job not submitted.\nThe other absolutely necessary argument for submitting a job to the Farm is the -G groupname variable. This specifies to which group at the Sanger you’re billing the compute resources. We’ll be using a temporary testing group for this course called farm-course, but once you settle into a lab you’ll use their own group name.\nLet’s try adding it to the bsub argument list:\nbsub -o simple_job.out -G farm-course job_scripts/simple_job.sh\nIf it was submitted correctly, we should see this message:\nJob &lt;xxxxxx&gt; is submitted to default queue &lt;normal&gt;.\nOnce the job is finished, we can investigate the output by looking inside the file, for example cat simple_job.out.\n\n\n\n\n\n\nNote\n\n\n\nThe first line of the shell scripts #!/bin/bash is called a shebang and indicates which program should interpret this script. In this case, bash is the interpreter of shell scripts (there’s other shell interpreters, but that’s beyond what we need to worry about here).\nRemember to always have this as the first line of your script. If you don’t, bsub will throw an error.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LSF Scheduler</span>"
    ]
  },
  {
    "objectID": "materials/04-lsf.html#configuring-job-options",
    "href": "materials/04-lsf.html#configuring-job-options",
    "title": "6  LSF Scheduler",
    "section": "6.3 Configuring Job Options",
    "text": "6.3 Configuring Job Options\nThe -o argument is just one of over 70 different options for submitting a job with bsub. You can imagine the bsub command would get rather long and difficult to keep track of! To make submitting a job simpler and more reproducible, you can include each of your bsub arguments as a line starting with #BSUB at the beginning of your script within the script, after the shebang.\nHere is how we could modify our script:\n#!/bin/bash\n#BSUB -o job_logs/simple_job.out\n#BSUB -G farm-course\n\nsleep 8 # hold for 8 seconds\necho \"This job is running on:\"\nhostname\nIf we now re-run the script using bsub simple_job.sh, the output goes to a file within the logs folder named simple_job.out.\nThere are many other options we can specify when using LSF, and we will encounter several more of them as we progress through the materials. Here are some of the most common ones (anything in &lt;&gt; is user input):\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\n-cwd &lt;path&gt;\nworking directory used for the job. This is the directory that LSF will use as a reference when running the job.\n\n\n-o &lt;path/filename&gt;\nfile where the output that would normally be printed on the console is saved in. This is defined relative to the working directory set above.\n\n\n-e &lt;path/filename&gt;\nfile where the error log is saved in. This is defined relative to the working directory set above. If you don’t specify an error file, the error log will write to the -o output file.\n\n\n-G &lt;name&gt;\ngroup name. This is required on the farm as it logs compute resources used for billing to your group. Ask your labmates for the name.\n\n\n-q &lt;name&gt;\npartition name. See details in the following section.\n\n\n- n &lt;ncores&gt;\nnumber of CPUs to be requested.\n\n\n-R \"select[mem&gt;&lt;megabytes_required&gt;] rusage[mem=&lt;megabytes_required&gt;]\"\nFirst part of two options to request custom RAM memory for the job.\n\n\n-M&lt;megabytes_required&gt;\nSecond part of two options to request custom RAM memory for the job.\n\n\n-W&lt;time in the form of [hour:]minute&gt;\nthe time you need for your job to run. This is not always easy to estimate in advance, so if you’re unsure you may want to request a good chunk of time. However, the more time you request for your job, the lower its priority in the queue.\n\n\n-J &lt;name&gt;\na name for the job.\n\n\n\n\n\n\n\n\n\nDefault Resources\n\n\n\nIf you don’t specify any options when submitting your jobs, you will get the default configured by the HPC admins. For example, on farm22, the defaults you will get are:\n\n10 minutes of running time (equivalent to -W10)\nnormal partition (equivalent to -q normal)\n1 CPU (equivalent to -n 1)\n100MB RAM (equivalent to -M100 -R \"select[mem&gt;100] rusage[mem=100]\")\n\n\n\n\n6.3.1 Partitions/Queues\nOften, HPC servers have different types of compute node setups (e.g. partitions for fast jobs, or long jobs, or high-memory jobs, etc.). LSF calls these “queues” and you can use the -q option to choose which queue your job runs on. Usually, which queues are available on your HPC should be provided by the admins.\nIt’s worth keeping in mind that these partitions have separate queues, so you should always try to choose the partition that is most suited to your job. You can check the queues available using the command bqueues -l. Here are some of the partitions available on farm22:\n\nGeneral use partitions:\n\nnormal partition (default) with a maximum of 12 hours\nlong partition with a maximum of 48 hours\nbasement partition with a maximum of 30 days; only 300 basement jobs are allowed per user simultaneously.\n\nSpecial case partitions:\n\nhugemem/hugemem-restricted/teramem queues for large memory machines (512GB/1TB).\nyesterday partition for very urgent jobs that need to be done “yesterday”; only 7 jobs are allowed per user simultaneously.\nsmall partition for many, very small jobs (batches 10 jobs together to prevent scheduler overload).",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LSF Scheduler</span>"
    ]
  },
  {
    "objectID": "materials/04-lsf.html#getting-job-information",
    "href": "materials/04-lsf.html#getting-job-information",
    "title": "6  LSF Scheduler",
    "section": "6.4 Getting Job Information",
    "text": "6.4 Getting Job Information\nAfter submitting a job, we may want to know:\n\nWhat is going on with my job? Is it running, has it finished?\nIf it finished, did it finish successfully, or did it fail?\nHow many resources (e.g. RAM) did it use?\nWhat if I want to cancel a job because I realised there was a mistake in my script?\n\nYou can check the status of all your jobs in the queue by using:\nbjobs -w\nOr get detailed information on one job in particular with:\nbjobs -l &lt;JOBID&gt;\nThis gives you information about the job’s status while it’s running: PEND means it’s pending (waiting in the queue) and RUN means it’s running.\nOnce the job is complete, you can still use bjobs -l &lt;JOBID&gt; to get job statistics. Alternatively, you may find it easier to use bhist, as it includes time and memory usage in a more reader-friendly format:\nbhist -l JOBID\nThis shows you the status of the job, whether it completed or not, how long it took to run, and how much memory it used. Therefore, this command is very useful to determine suitable resources (e.g. RAM, time) next time you run a similar job.\nAlternatively, you can use the bacct command once a job has completed, which allows displaying this and other information in a more condensed way (and for multiple jobs if you want to).\nFor example:\nbacct JOBID\nwill give you information about one specific job\nYou can add other options to the bacct command to glean more or less information with:\n\n-l for extra information about the job\n-b for brief information about the job\n\nYou can also select groups of jobs based on certain characteristics, like:\n\n-q &lt;partition&gt; to select all jobs you’ve run in a certain partition\n-d to select all jobs that have completed successfully\n-e to select all jobs that had end status of EXIT (failed)\n-x to select jobs that raised an exception while running\n\nAs a rule, running bacct without the -l option results in aggregate job statistics for the jobs included, while with the -l option results in a long list of separate, per-job statistics.\nAll the options available with bacct can be listed using bacct -h. If you forgot what the job id is, check the standard output file that was specified with the -o argument of bsub.\n\n\n\n\n\n\nNote\n\n\n\nThe bacct command may not be available on every HPC, as it depends on how it was configured by the admins.\nOn the Farm HPC, bacct is reading information from /usr/local/job_scripts/work/&lt;cluster_name&gt;/logdir/lsb.acct.*.\n\n\nFinally, if you want to suspend a job, you can use:\nbstop &lt;JOBID&gt;\nSuspended jobs can be restarted using:\nbresume &lt;JOBID&gt;\nTo irreversibly end a job, use:\nbkill &lt;JOBID&gt;\nAll three commands bstop, bresume, and bkill can be applied to all of your own jobs by replacing the job ID with 0.\nIt’s impossible to edit other users’ jobs, so don’t worry about accidentally deleting everyone’s Farm jobs!\n\nWATCH OUT\nWhen specifying the -o option including a directory name, if the output directory does not exist, bjobs will still run, but produce no output file.\nFor example, let’s say that we would like to keep our job output files in a folder called “logs”. For the example above, we might set these #BSUB options:\n#BSUB -cwd /nfs/users/nfs_USERINITIAL/USERID/hpc_workshop/\n#BSUB -o job_logs/simple_job.log\nBut, unless we create the job_logs/ directory before running the job, bjobs will not produce a file of standard output.\nAnother thing to note is that you should not use the ~ home directory shortcut with the -cwd option. For example:\n#BSUB -cwd ~/hpc_workshop/\nwill not reliably work. Instead you should use the full path as shown above.\n\n\n6.4.1 Exercise\n\n\n\n\n\n\nExercise 1 - Submiting jobs with LSF\n\n\n\n\n\n\nIn the “analysis_scripts” directory, you will find an R script called pi_estimator.R. This script tries to get an approximate estimate for the number Pi using a stochastic algorithm.\n\n\nHow does the algorithm work?\n\nIf you are interested in the details, here is a short description of what the script does:\n\nThe program generates a large number of random points on a 1×1 square centered on (½,½), and checks how many of these points fall inside the unit circle. On average, π/4 of the randomly-selected points should fall in the circle, so π can be estimated from 4f, where f is the observed fraction of points that fall in the circle. Because each sample is independent, this algorithm is easily implemented in parallel.\n\n\n\n\nEstimating Pi by randomly placing points on a quarter circle. (Source: HPC Carpentry)\n\n\n\nR scripts can be run from the terminal using the Rscript command line interpreter, for example Rscript analysis_scripts/pi_estimator.R. However, as we are working on a HPC, we use a shell script to submit this command to the job scheduler.\nBut first, we need to install a required package - argparse - into the Farm’s R environment.\nPause here to work through the below instructions.\n\nInstalling required R packages\nThe pi_estimator.R script requires the R library argparse, which must first be installed into your R workspace. Because you’re using R on this HPC for the first time, you’ll need to install the package to run the script successfully. You should only have to run this once:\n\nOpen an R console by typing R\nRun the command install.packages('argparse', repos = 'https://cloud.r-project.org')\n\nType yes if asked to create a personal library\nType yes if asked to approve the default library location\n\nOnce installation is complete, check to make sure it installed properly by loading the package: library(argparse)\n\nIf there are no errors, the R library has been successfully installed!\n\nTo exit the R console run q(\"no\") (quit without saving)\n\n\nSubmitting the R script as a LSF job\nNow we’re ready to run the estimate_pi.sh script.\n\nEdit the shell script in job_scripts/estimate_pi.sh by correcting the code where the word “FIXME” appears. Submit the job to LSF and check its status in the queue.\nHow long did the job take to run?\n\n\nHint\n\nUse bhist -l JOBID or bacct -l JOBID.\n\nThe number of samples used to estimate Pi can be modified using the --nsamples option of our script, defined in millions. The more samples we use, the more precise our estimate should be.\n\nAdjust your LSF submission script to use 200 million samples (Rscript analysis_scripts/pi_estimator.R --nsamples 200), and save the job output in job_logs/estimate_pi_200M.out and job_logs/estimate_pi_200M.err.\nMonitor the job status with bjobs and bhist -l JOBID. Review the output files in your logs folder. Do you find any issues?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA1.\nIn the shell script we needed to correct the user-specific details in the #BSUB options. Also, we needed to specify the path to the script we wanted to run. This can be defined relative to the working directory that we’ve set with -cwd. For example:\n#!/bin/bash\n#BSUB -q normal  # name of the queue to run job on\n#BSUB -cwd /nfs/users/nfs_USERINITIAL/USERID/hpc_workshop  # working directory\n#BSUB -o job_logs/estimate_pi.out  # standard output file\n#BSUB -e job_logs/estimate_pi.err  # standard error file\n#BSUB -n 1        # number of CPUs. Default: 1\n#BSUB -R \"select[mem&gt;1000] rusage[mem=1000]\" # RAM memory part 1. Default: 100MB\n#BSUB -M1000  # RAM memory part 2. Default: 100MB\n\n# run the script\nRscript analysis_scripts/pi_estimator.R\nWith the corrected script, we can submit the job:\nbsub job_scripts/estimate_pi.sh\nA2.\nAs suggested in the hint, we can use the bhist or bacct commands for this:\nbhist -l JOBID\nbacct -l JOBID\nReplacing JOBID with the ID of the job we just ran.\nIf you cannot remember what the job id was, you can check all your jobs using bjobs or view the outfile generated at the beginning of the run.\nSometimes it may happen that the “Memory Usage” (or MEM) is reported as 0M or a lower value than you would expect. That’s very odd, since for sure our script must have used some memory to do the computation. The reason is that LSF doesn’t always have time to pick memory usage spikes, and so it reports a zero. This is usually not an issue with longer-running jobs.\nA3.\nThe modified script should look similar to this:\n#!/bin/bash\n#BSUB -q normal  # name of the partition to run job on\n#BSUB -G farm-course # groupname for billing\n#BSUB -cwd /nfs/users/nfs_USERINITIA/USERID/hpc_workshop  # working directory\n#BSUB -e job_logs/estimate_pi_200M.err  # standard error file\n#BSUB -n1        # number of CPUs. Default: 1\n#BSUB -R \"select[mem&gt;1000] rusage[mem=1000] span[hosts=1]\" # RAM memory part 1. Default: 100MB\n#BSUB -M1000  # RAM memory part 2. Default: 100MB\n\n# run the script\nRscript analysis_scripts/pi_estimator.R --nsamples 200\nAnd then send the job to the job scheduler:\nbsub job_scripts/estimate_pi.sh\nHowever, when we run this job, examining the output file (cat job_logs/estimate_pi_200M.out) will reveal and error indicating that our job was killed. There are few clues for this, most obviously this note:\nTERM_MEMLIMIT: job killed after reaching LSF memory usage limit.\nExited with exit code 1.\nFurthermore, if we use bjobs to get information about the job, it will show EXIT as the status instead of DONE.\nWe can also check this by seeing what bacct -l reports as “Memory Utilized” and see that it used 100% of the memory we gave the job. There is also this table in the output of this command:\nShare group charged &lt;/WTSI/other/farm-course/ht10&gt;\n     CPU_T     WAIT     TURNAROUND   STATUS     HOG_FACTOR    MEM    SWAP\n     10.71        1             13     exit         0.8240   2.1G      0M\n     CPU_PEAK     CPU_EFFICIENCY      MEM_EFFICIENCY\n      0.83                83.33%             220.00%\nWe can see that our job tried to use at least 2.1G (maybe it would have needed even more), but we only requested 1000M (~1G). So, that explains why it was killed!\nTo correct this problem, we would need to increase the memory requested to LSF, adding to our script, for example, #BSUB -R \"select[mem&gt;30000] rusage [mem=30000] span[hosts=1]\" and #BSUB -M 30000 to request 30Gb of RAM memory for the job.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LSF Scheduler</span>"
    ]
  },
  {
    "objectID": "materials/04-lsf.html#lsf-environment-variables",
    "href": "materials/04-lsf.html#lsf-environment-variables",
    "title": "6  LSF Scheduler",
    "section": "6.5 LSF Environment Variables",
    "text": "6.5 LSF Environment Variables\nOne useful feature of LSF jobs is the automatic creation of environment variables. Generally speaking, variables store a value within them, and can either be created by us, or sometimes they are automatically created by programs or available by default in our shell.\n\n\n\n\n\n\nMore about shell variables (click to view)\n\n\n\n\n\nAn example of a common shell environment variable is $HOME, which stores the path to the user’s /home directory. We can print the value of a variable with echo $HOME.\nThe syntax to create a variable ourselves is:\nVARIABLE=\"value\"\nNotice that there should be no space between the variable name and its value.\nIf you want to create a variable with the result of evaluating a command, then the syntax is:\nVARIABLE=$(command)\nTry these examples:\n# Make a variable with a path starting from the user's /home\nDATADIR=\"$HOME/hpc_workshop/data/\"\n\n# list files in that directory\nls $DATADIR\n\n# create a variable with the output of that command\nDATAFILES=$(ls $DATADIR)\nSee their values:\necho $DATADIR\necho $DATAFILES\n\n\n\nWhen you submit a job with LSF, it creates several variables, all starting with the prefix $LSB_. One useful variable is $LSB_MAX_NUM_PROCESSORS, which stores how many CPUs we requested for our job. This means that we can use the variable to automatically set the number of CPUs for software that support multi-threading. We will see an example in Exercise 2.\nHere is a table summarising some of the most useful environment variables that LSF creates:\n\n\n\nVariable\nDescription\n\n\n\n\n$LSB_MAX_NUM_PROCESSORS\nNumber of CPUs requested with -n\n\n\n$LSB_JOBID\nThe job ID\n\n\n$LSB_JOBNAME\nThe name of the job defined with -J\n\n\n$LSB_EXECCWD\nThe working directory defied with -cwd\n\n\n$LSB_JOBINDEX\nThe number of the sub-job when running parallel arrays (covered in the Job Arrays section)\n\n\n\n\n6.5.1 Exercise\n\n\n\n\n\n\nExercise 2 - Using LSF environment variables\n\n\n\n\n\n\nThe R script used in the previous exercise supports parallelisation of some of its internal computations. The number of CPUs used by the script can be modified using the --ncpus option. For example pi_estimator.R --nsamples 200 --ncpus 2 would use two CPUs.\n\nModify your submission script (job_scripts/estimate_pi.sh) to: \n\nUse the $LSB_MAX_NUM_PROCESSORS variable to set the number of CPUs used by pi_estimator.R (and ensure you have set --nsamples 200 as well).\nRequest 10G of RAM memory for the job.\nBonus (optional): use echo within the script to print a message indicating the job number (LSF’s job ID is stored in the variable $LSB_JOBID).\n\nSubmit the job three times, each one using 1, 2 and then 8 CPUs. Make a note of each job’s ID.\nCheck how much time each job took to run (using bhist JOBID or bacct JOBID). Did increasing the number of CPUs shorten the time it took to run?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA1.\nWe can modify our submission script in the following manner, for example for using 2 CPUs:\n#!/bin/bash\n#BSUB -q normal     # partiton name\n#BSUB -D /nfs/users/user_USERINITIAL/USERID/hpc_workshop/  # working directory\n#BSUB -o job_logs/estimate_pi_200M_2cpu.out      # output file\n#BSUB -e job_logs/estimate_pi_200M_2cpu.err      # error file\n#BSUB -R \"select[mem&gt;10000] rusage[mem=10000]\" # RAM memory part 1. Default: 100MB\n#BSUB -M10000  # RAM memory part 2. Default: 100MB\n#BSUB -n 2                          # number of CPUs\n\n# launch the Pi estimator script using the number of CPUs that we are requesting from LSF\nRscript analysis_scripts/pi_estimator.R --nsamples 200 --ncpus $LSB_MAX_NUM_PROCESSORS\n\n# echo number of CPUS\necho \"Processors used: $LSB_MAX_NUM_PROCESSORS\"\nYou can then run the script using this command:\nbsub job_scripts/estimate_pi.sh\nWe can run the job multiple times while modifying the #BSUB -n option, saving the file and re-running bsub job_scripts/estimate_pi.sh.\nAfter running each job we can use bhist JOBID or bjobs -l JOBID or bacct JOBID commands to obtain information about how long it took to run.\nIn this case, it does seem that increasing the number of CPUs shortens the time the job takes to run. However, the increase is not linear at all. For example going from 1 to 2 CPUs seems to make the job run faster, however increasing to 8 CPUs makes little difference compared to 2 CPUs (this may depend on how many --nsamples you used). This is possibly because there are other computational costs to do with this kind of parallelisation (e.g. keeping track of what each parallel thread is doing).",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LSF Scheduler</span>"
    ]
  },
  {
    "objectID": "materials/04-lsf.html#interactive-login",
    "href": "materials/04-lsf.html#interactive-login",
    "title": "6  LSF Scheduler",
    "section": "6.6 Interactive Login",
    "text": "6.6 Interactive Login\nSometimes it may be useful to directly get a terminal on one of the compute nodes. This may be useful, for example, if you want to test some scripts or run some code that you think might be too demanding for the login node (e.g. to compress some files).\nIt is possible to get interactive access to a terminal on one of the compute nodes using the -Is argument in bsub. This command takes options similar to the normal bsub program, so you can request resources in the same way you would when submitting scripts.\nFor example, to access to 8 CPUs and 10GB of RAM for 10 minutes on one of the compute nodes we would do:\nbsub -G farm-course -Is -n8 -R \"select[mem&gt;1000] rusage[mem=1000]\" -M1000 -q normal -W10 bash\nYou may get a message saying that LSF is waiting to allocate your request (you go in the queue, just like any other job!). Eventually, when you get in, you will notice that your terminal will indicate you are on a different node: for example, instead of farm22-head2 you may be in node-5-10-4. You can check which compute node you’re in with hostname.\nAfter you’re in, you can run any commands you wish, without worrying about affecting other users’ work. Once you are finished, you can use the command exit (or Ctrl + D) to terminate the session, and you will go back to the login node.\nNote that, if the time you requested (with the -W option) runs out, your session will be immediately killed.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LSF Scheduler</span>"
    ]
  },
  {
    "objectID": "materials/04-lsf.html#summary",
    "href": "materials/04-lsf.html#summary",
    "title": "6  LSF Scheduler",
    "section": "6.7 Summary",
    "text": "6.7 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nInclude the commands you want to run on the HPC in a shell script.\n\nAlways remember to include #!/bin/bash as the first line of your script.\n\nSubmit jobs to the scheduler using bsub submission_script.sh.\nCustomise the jobs by including #BSUB options at the top of your script (see table in the materials above for a summary of options).\n\nAs a good practice, always define an output file with #BSUB -o. All the information about the job will be saved in that file, including any errors.\n\nCheck the status of a submitted job by using bjobs. You can get detailed information about a job (such as the time it took to run or how many resources it used) using bjobs -l JOBID or bacct -l JOBID or bhist JOBID.\nTo cancel a running job use bkill JOBID.\n\n\nFurther resources\n\nIBM Spectrum LSF command reference\nbsub Reference Page\nLSF to PBS/SLURM/SGE/LoadLeveler schedulers",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>LSF Scheduler</span>"
    ]
  },
  {
    "objectID": "materials/05-software.html",
    "href": "materials/05-software.html",
    "title": "7  Software Management",
    "section": "",
    "text": "7.1 Using Pre-installed Software\nIt is very often the case that HPC admins have pre-installed several software packages that are regularly used by their users. Because there can be a large number of packages (and often different versions of the same program), you need to load the programs you want to use in your script using the module tool.\nThe following table summarises the most common commands for this tool:\nIf a package is not available through the module command, you can contact the HPC admin and ask them to install it for you. Alternatively, you can use a package manager as we show in the next section.\nThe packages you have available will depend on the group you’re in. Your group affects your $MODULEPATH which in turn gives you access to various programme-specific packages.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Software Management</span>"
    ]
  },
  {
    "objectID": "materials/05-software.html#using-pre-installed-software",
    "href": "materials/05-software.html#using-pre-installed-software",
    "title": "7  Software Management",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\nmodule avail\nList all available packages.\n\n\nmodule avail -i --contains \"pattern\"  or  module avail 2&gt;&1 | grep -i \"pattern\"\nSearch the available package list that matches “pattern” in a case-insensitive way.\n\n\nmodule load &lt;program&gt;\nLoad the program and make it available for use.\n\n\nmodule unload &lt;program&gt;\nUnload the program (removes it from your PATH).\n\n\n\n\n\n\n\n7.1.1 Exercise\n\n\n\n\n\n\nExercise 1 - Software modules\n\n\n\n\n\n\nLet’s load the software package bowtie2 so we can index and align a drosophila genome.\n\nCheck what modules are available on gen22. Can you find bowtie version 2.4.2?\nLoad the bowtie2 version 2.4.2 using module load.\nRun bowtie2 --help to make sure that the module has loaded properly.\nFigure out where the bowtie2 software package is stored with the which command.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA1.\nUse module avail to list all software packages available on gen22.\nA2.\nWe can first search to see which (if any) modules are available for this software:\nmodule avail -i --contains bowtie\nWe seem to have a few versions available:\nbowtie-1.3.1\nbowtie2-2.5.2/python-3.12.0\nHGI/softpack/groups/team354/bowtie2/2.4.2\nISG/experimental/sp39/bowtie/2.5.2\nAs we are interested in version 2.4.2, we can load the module with:\nmodule load HGI/softpack/groups/team354/bowtie2/2.4.2\nA3\nRunning bowtie2 --help should print the help page from the software. If this doesn’t come up or if an error appears, you likely didn’t manage to load the module properly. This is a good sanity check when loading modules.\nA4\nEntering which bowtie2 should show a path to /software/hgi/softpack/installs/groups/team354//bowtie2/2.4.2-scripts/bowtie2.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Software Management</span>"
    ]
  },
  {
    "objectID": "materials/05-software.html#example-sequence-read-alignment",
    "href": "materials/05-software.html#example-sequence-read-alignment",
    "title": "7  Software Management",
    "section": "7.2 Example: sequence read alignment",
    "text": "7.2 Example: sequence read alignment\nOnce your sequencing data have been processed by SequenceSpace and/or your programme’s IT team, you can then begin to analyze them in lustre. First, you need to transfer them from the “read-only” NFS or iRODS sections of the Farm to your lustre workspace.\nWe won’t be doing this today (this is often a programme-specific process that requires unique permissions and some extra training), but it’s important to keep this larger structure of data workflows in mind. When discussing what you need to get started in your group, be sure to ask about how to get added to the group’s permissions list and how they usually access sequencing data (iRODS, Canapps, NFS, etc.)\nFor our genome alignment Exercise 2, we’ll pretend that you have already staged the raw data files from long term storage (iRODS or NFS) into your lustre storage location in a folder called data within hpc_workshop.\n\n7.2.1 Exercise\n\n\n\n\n\n\nExercise 2 - Using modules with LSF\n\n\n\n\n\n\nIn the hpc_workshop/data folder, you will find some files resulting from whole-genome sequencing individuals from the model organism Drosophila melanogaster (fruit fly). Our objective will be to align our sequences to the reference genome, using a software called bowtie2.\n\nBut first, we need to prepare our genome for this alignment procedure (this is referred to as indexing the genome). We have a file with the Drosophila genome in data/genome/drosophila_genome.fa.\n\nOpen the script in job_scripts/drosophila_genome_indexing.sh using the nano text editor (or another editor of your choice).\nEdit the #BSUB options that contain the word “FIXME”. Save the script.\nSubmit the script to LSF using bsub, check it’s progress, and whether it ran successfully.\n\nTroubleshoot any issues that may arise.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nWe need to fix the script to specify the correct working directory with our username (only showing the relevant line of the script):\n#BSUB -cwd /nfs/users/nfs_USERINITIAL/USERID/hpc_workshop/\nWe also need to make sure we load the module in the compute node, by adding the following line at the start of the script:\nmodule load HGI/softpack/groups/team354/bowtie2/2.4.2\nRemember that even though we may have loaded the environment on the login node, the scripts are run on a different machine (one of the compute nodes), so we need to remember to always load modules or conda environments in our LSF submission scripts.\nWe can then launch it with bsub:\nbsub job_scripts/drosophila_genome_indexing.sh\nWe can check the job status by using bjobs. And we can obtain more information by using bacct JOBID or bhist JOBID.\nWe should get several output files in the directory results/drosophila/genome with an extension “.bt2”:\nls results/drosophila/genome\nindex.1.bt2\nindex.2.bt2\nindex.3.bt2\nindex.4.bt2\nindex.rev.1.bt2\nindex.rev.2.bt2",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Software Management</span>"
    ]
  },
  {
    "objectID": "materials/05-software.html#package-managers",
    "href": "materials/05-software.html#package-managers",
    "title": "7  Software Management",
    "section": "7.3 Package managers",
    "text": "7.3 Package managers\nOften you may want to use software packages that are not be installed by default on the HPC. There are several ways you could manage your own software installation, one of the most popular ones being the use of the package manager Conda or its newer implementation Mamba.\nCovering Conda/Mamba is out of the scope of these materials, but check out our course Reproducible and scalable bioinformatics: managing software and pipelines to learn more about this topic.\nIf you are familiar with Conda/Mamba, you may know that to activate a software environment you use the command mamba activate. However, to load environments in a shell script that is being submitted to LSF you need to first source a configuration file from Conda/Mamba. For example, let’s say we had an environment called datasci; to activate it in our LSF script, we would need the following syntax:\n# Always add these two commands to your scripts\neval \"$(conda shell.bash hook)\"\nsource $CONDA_PREFIX/etc/profile.d/mamba.sh\n\n# then you can activate the environment\nmamba activate datasci\nThis is because when we submit jobs to LSF the jobs will start in a non-interactive shell, and mamba doesn’t get automatically set. Running the source command shown will ensure mamba activate becomes available.\n\n\n\n\n\n\nMamba versus Module\n\n\n\nAlthough Mamba is a great tool to manage your own software installation, the disadvantage is that the software is not compiled specifically taking into account the hardware of the HPC. This is a slightly technical topic, but the main practical consequence is that software installed by HPC admins and made available through the module system may sometimes run faster than software installed via conda. This means you will use fewer resources and your jobs will complete faster.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Software Management</span>"
    ]
  },
  {
    "objectID": "materials/05-software.html#summary",
    "href": "materials/05-software.html#summary",
    "title": "7  Software Management",
    "section": "7.4 Summary",
    "text": "7.4 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nThe module tool can be used to search for and load pre-installed software packages on a HPC. This tool may not always be available on your HPC.\n\nmodule avail is used to list the available software.\nmodule load PACKAGE_NAME is used to load the package.\n\nTo install your own software, you can use the Conda package manager.\n\nConda allows you to have separate “software environments”, where multiple package versions can co-exist on your system.\n\nUse conda env create &lt;ENV&gt; to create a new software environment and conda install -n &lt;ENV&gt; &lt;PROGRAM&gt; to install a program on that environment.\nUse conda activate &lt;ENV&gt; to “activate” the software environment and make all the programs installed there available.\n\nWhen submitting jobs to bsub, always remember to include source $CONDA_PREFIX/etc/profile.d/conda.sh at the start of the shell script, followed by the conda activate command.\n\nAlways remember to include either module load or conda activate in your submission script.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Software Management</span>"
    ]
  },
  {
    "objectID": "materials/06-arrays.html",
    "href": "materials/06-arrays.html",
    "title": "8  Job Parallelisation",
    "section": "",
    "text": "8.1 Parallelising Tasks\nOne of the important concepts in the use of a HPC is parallelisation. This concept is used in different ways, and can mean slightly different things.\nA program may internally support parallel computation for some of its tasks, which we may refer to as multi-threading or multi-core processing. In this case, there is typically a single set of “input -&gt; output”, so all the parallel computations need to finish in order for us to obtain our result. In other words, there is some dependency between those parallel calculations.\nOn the other hand, we may want to run the same program on different inputs, where each run is completely independent from the previous run. In these cases we say the task is “embarrassingly parallel”. Usually, running tasks completely in parallel is faster, since we remove the need to keep track of what each task’s status is (since they are independent of each other).\nFinally, we may want to do both things: run several jobs in parallel, while each of the jobs does some internal parallelisation of its computations (multi-threading).",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Job Parallelisation</span>"
    ]
  },
  {
    "objectID": "materials/06-arrays.html#parallelising-tasks",
    "href": "materials/06-arrays.html#parallelising-tasks",
    "title": "8  Job Parallelisation",
    "section": "",
    "text": "Schematic of parallelisation.\n\n\n\n\n\n\n\n\nTerminology Alert!\n\n\n\nSome software packages have an option to specify how many CPU cores to use in their computations (i.e. they can parallelise their calculations). However, in their documentation this may be referred to as cores, processors, CPUs or threads, which are used more or less interchangeably to essentially mean “how many calculations should I run in parallel?”. Although these terms are technically different, when you see this mentioned in the software’s documentation, usually you want to set it as the number of CPU cores you request from the cluster.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Job Parallelisation</span>"
    ]
  },
  {
    "objectID": "materials/06-arrays.html#job-arrays",
    "href": "materials/06-arrays.html#job-arrays",
    "title": "8  Job Parallelisation",
    "section": "8.2 Job Arrays",
    "text": "8.2 Job Arrays\nThere are several ways to parallelise jobs on a HPC. One of them is to use a built-in functionality in LSF called job arrays.\nJob arrays are a collection of jobs that run in parallel with identical parameters. Any resources you request (e.g. -n, -R, -M -W) apply to each individual job of the “array”. This means that you only need to submit one “master” job, making it easier to manage and automate your analysis using a single script.\nJob arrays are created with the -J option -J arrayName[start-finish] where arrayName becomes the Job Name and start and finish are integers defining the range of array numbers created by LSF. For example, setting -J testJob[1-3] would result in three jobs sent: testJob[1], testJob[2], and testJob[3].\nIf you’d like to specify the -J option within the script header with #BSUB, it needs to be one of the first arguments listed. We recommend placing it as the third, below -G and -q options.\nWith this array list option within -J, LSF then creates a special shell variable $LSB_JOBINDEX, which contains the array number for the job being processed. Later in this section we will see how we can use some tricks with this variable to automate our analysis.\nFor now let’s go through this simple example, which shows what a job array looks like (you can find this script in the course folder job_scripts/parallel_arrays.sh):\n# ... some lines omitted ...\n#BSUB -J parallel[1-3]\n#BSUB -o job_logs/parallel_arrays_%I.out\n#BSUB -e job_logs/parallel_arrays_%I.err\n\n\necho \"This is task number $LSB_JOBINDEX\"\necho \"Using $LSB_MAX_NUM_PROCESSORS CPUs\"\necho \"Running on:\"\nhostname\nSubmitting this script with bsub job_scripts/parallel_arrays.sh will launch 3 jobs.\nThe %I keyword is used in our output filename (-o and -e) and will be replaced by the array number, so that we end up with three files: parallel_arrays_1.out, parallel_arrays_2.out and parallel_arrays_3.out.\nYou could optionally also include the %J keyword to add the Job ID number to the file name.\nYou can investigate the results using head -n 4 job_logs/parallel_arrays*out.\nLooking at the output in those files should make it clearer that $LSB_JOBINDEX stores the array number of each job, and that each of them uses 2 CPUS (-n2 option). The compute node that they run on may be variable (depending on which node was available to run each job).\n\n\n\n\n\n\nDefining non-sequential job arrays\n\n\n\nYou can define job array numbers in multiple ways, not just sequentially.\nHere are some examples:\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\n-J jobName[0-31]\nindex values between 0 and 31\n\n\n-J jobName[1,3,5,7]\nindex values of 1, 3, 5 and 7\n\n\n-J [1-100]%10\nindex values between 1 and 100, batching 10 elements to run at the same time.\n\n\n\n\n\n\n8.2.1 Exercise\n\n\n\n\n\n\nExercise 1 - Replicate a job with arrays\n\n\n\n\n\n\nPreviously, we used the pi_estimator.R script to obtain a single estimate of the number Pi. Since this is done using a stochastic algorithm, we may want to run it several times to get a sense of the error associated with our estimate.\n\nUse nano to open the LSF submission script in job_scripts/parallel_estimate_pi.sh.\nAdjust the #BSUB options (where word “FIXME” appears), to run the job 10 times using a job array.\nLaunch the job with bsub, monitor its progress and examine the output.\n\n\nHint\n\nNote that the output of pi_estimator.R is now being sent to individual text files to the directory results/pi/.\n\nBonus: combine all the output files into a single file. Should you run this operation directly on the login node, or submit it as a new job to LSF?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA1.\nIn our script, we need to add #BSUB -J parallelEst[1-10] as one of our options, so that when we submit this script to bsub, it will run 10 iterations of it in parallel.\nAlso, remember to edit LSF’s working directory with your username, at the top of the script in the #bsub -cwd option.\nA2.\nWe can launch our adjusted script with bsub job_scripts/parallel_estimate_pi.sh. When we check our jobs with bjobs, we will notice several jobs with JOBID in the format “ID[1]”, “ID[2]”, etc. These indicate the number of the array that is currently running as part of that job submission.\nIn this case, we will get 10 output log files, each with the job array number at the end of the filename (we used the %I keyword in the #BSUB -o option to achieve this).\nThe 10 separate estimates of Pi were written to separate text files named results/pi/replicate_1.txt, results/pi/replicate_2.txt, etc.\nA3.\nTo combine the results of these 10 replicate runs of our Pi estimate, we could use the Unix tool cat:\ncat results/pi/replicate_*.txt &gt; results/pi/combined_estimates.txt\nIf we examine this file (e.g. with less results/pi/combined_estimates.txt) we can see it has the results of all the runs of our simulation.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Job Parallelisation</span>"
    ]
  },
  {
    "objectID": "materials/06-arrays.html#using-lsb_jobindex-to-automate-jobs",
    "href": "materials/06-arrays.html#using-lsb_jobindex-to-automate-jobs",
    "title": "8  Job Parallelisation",
    "section": "8.3 Using $LSB_JOBINDEX to Automate Jobs",
    "text": "8.3 Using $LSB_JOBINDEX to Automate Jobs\nOne way to automate our jobs is to use the job array number (stored in the $LSB_JOBINDEX variable) with some command-line tricks. The trick we will demonstrate here is to parse a CSV file to read input parameters for our scripts.\nFor example, in our data/ folder we have the following file, which includes information about the samples we want to process:\ncat data/drosophila_sample_info.csv\nsample,read1,read2\nSRR307023,data/reads/SRR307023_1.fastq.gz,data/reads/SRR307023_2.fastq.gz\nSRR307024,data/reads/SRR307024_1.fastq.gz,data/reads/SRR307024_2.fastq.gz\nSRR307025,data/reads/SRR307025_1.fastq.gz,data/reads/SRR307025_2.fastq.gz\nSRR307026,data/reads/SRR307026_1.fastq.gz,data/reads/SRR307026_2.fastq.gz\nSRR307027,data/reads/SRR307027_1.fastq.gz,data/reads/SRR307027_2.fastq.gz\nSRR307028,data/reads/SRR307028_1.fastq.gz,data/reads/SRR307028_2.fastq.gz\nSRR307029,data/reads/SRR307029_1.fastq.gz,data/reads/SRR307029_2.fastq.gz\nSRR307030,data/reads/SRR307030_1.fastq.gz,data/reads/SRR307030_2.fastq.gz\nThis is a CSV (comma-separated values) format, with three columns named “sample”, “read1” and “read2”. Let’s say we wanted to obtain information for the 2nd sample, which in this case is in the 3rd line of the file (because of the column names header). We can get the top N lines of a file using the head command (we pipe the output of the previous cat command):\ncat data/drosophila_sample_info.csv | head -n 3\nThis gets us lines 1-3 of the file. To get just the information about that 2nd sample, we can now pipe the output of the head command to the command that gets us the bottom lines of a file tail:\ncat data/drosophila_sample_info.csv | head -n 3 | tail -n 1\nFinally, to separate the two values that are separated by a comma, we can use the cut command, which accepts a delimiter (-d option) and a field we want it to return (-f option):\ncat data/drosophila_sample_info.csv | head -n 3 | tail -n 1 | cut -d \",\" -f 1\nIn this example, we use comma as a delimiter field and obtained the first of the values after “cutting” that line.\nSchematically, this is what we’ve done:\n\nSo, if we wanted to use job arrays to automatically retrieve the relevant line of this file as its input, we could use head -n $LSB_JOBINDEX in our command pipe above. Let’s see this in practice in Exercise 2.\n\n8.3.1 Exercise\n\n\n\n\n\n\nExercise 2 - Using the array index to customise jobs\n\n\n\n\n\n\n\nContinuing from our previous exercise where we prepared our Drosophila genome for bowtie2, we now want to map each of our samples’ sequence data to the reference genome.\nLooking at our data directory (ls hpc_workshop/data/reads), we can see several sequence files in standard fastq format. These files come in pairs (with suffix “_1” and “_2”), and we have 8 different samples. Ideally we want to process these samples in parallel in an automated way.\nWe have also created a CSV file with three columns in the data directory. One column contains the sample’s name (which we will use for our output files) and the other two columns contain the path to the first and second pairs of the input files. With the information on this table, we should be able to automate our data processing using a LSF job array.\n\nUse nano to open the LSF submission script in job_scripts/parallel_drosophila_mapping.sh. The first few lines of the code are used to fetch parameter values from the CSV file, using the special $LSB_JOBINDEX variable. Fix the #BSUB -J option to get these values from the CSV file.\n\n\nHint\n\nThe array should have as many numbers as there are lines in our CSV file. However, make sure the array number starts at 2 because the CSV file has a header with column names.\n\nLaunch the job with bsub and monitor its progress (bjobs), whether it runs successfully (bacct), and examine the LSF output log files.\nExamine the output files in the results/drosophila/mapping folder. (Note: the output files are text-based, so you can examine them by using the command line program less, for example.)\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\nA1.\nOur array numbers should be: #BSUB -J drosophila[2-9]. We start at 2, because the parameter values start at the second line of the parameter file. We finish at 9, because that’s the number of lines in the CSV file.\nWe also need to adjust the head -n command a few lines down to pull the correct line according to the $LSB_JOBINDEX variable assigned to each job.\nA2.\nWe can submit the script with bsub job_scripts/parallel_drosophila_mapping.sh. While the job is running we can monitor its status with bjobs. We should see several jobs listed with IDs as JOBID[ARRAYID] format.\nBecause we used the %I keyword in our #BSUB -o option, we will have an output log file for each job of the array. We can list these log files with ls job_logs/drosophila_mapping_*.out (using the “*” wildcard to match any character). If we examine the content of one of these files (e.g. cat job_logs/drosophila_mapping_2.out), at the top we should only see the messages we printed with the echo commands. The actual output of the bowtie2 program is a file in SAM format, which is saved into the results/drosophila/mapping folder.\nA3.\nOnce all the array jobs finish, we should have 8 SAM files in ls results/drosophila/mapping. We can examine the content of these files, although they are not terribly useful by themselves. In a typical bioinformatics workflow these files would be used for further analysis, for example SNP-calling.",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Job Parallelisation</span>"
    ]
  },
  {
    "objectID": "materials/06-arrays.html#summary",
    "href": "materials/06-arrays.html#summary",
    "title": "8  Job Parallelisation",
    "section": "8.4 Summary",
    "text": "8.4 Summary\n\n\n\n\n\n\nKey Points\n\n\n\n\nSome tools internally parallelise some of their computations, which is usually referred to as multi-threading or multi-core processing.\nWhen computational tasks are independent of each other, we can use job parallelisation to make them more efficient.\nWe can automatically generate parallel jobs using LSF job arrays with the bsub option -J.\nLSF creates a variable called $LSB_JOBINDEX, which can be used to customise each individual job of the array.\n\nFor example we can obtain the input/output information from a simple configuration text file using some command line tools: cat config.csv | head -n $LSB_JOBINDEX | tail -n 1\n\n\n\nFurther resources\n\nLSF Job Array Documentation\nAdditional Array Documentation",
    "crumbs": [
      "Slides",
      "Working on a HPC",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Job Parallelisation</span>"
    ]
  },
  {
    "objectID": "materials/appendices/lsf_cheatsheet.html",
    "href": "materials/appendices/lsf_cheatsheet.html",
    "title": "Appendix A — LSF Quick Reference Guide",
    "section": "",
    "text": "A.1 LSF Commands\nThis page summarises the most relevant information to work with the HPC, to be used as a quick-reference guide.\nThis is used in the examples that follow:",
    "crumbs": [
      "Slides",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>LSF Quick Reference Guide</span>"
    ]
  },
  {
    "objectID": "materials/appendices/lsf_cheatsheet.html#lsf-commands",
    "href": "materials/appendices/lsf_cheatsheet.html#lsf-commands",
    "title": "Appendix A — LSF Quick Reference Guide",
    "section": "",
    "text": "Command\nDescription\n\n\n\n\nbsub simulation.sh\nsubmit script to scheduler\n\n\nbjobs\njobs currently in the queue\n\n\nbkill JOBID\ncancel the job with the specified ID (get the ID from the command above)\n\n\nbkill -u xyz123\ncancel all your jobs at once\n\n\nbhist JOBID\nbasic information about the job\n\n\nbacct JOBID\ncustom information about your job",
    "crumbs": [
      "Slides",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>LSF Quick Reference Guide</span>"
    ]
  },
  {
    "objectID": "materials/appendices/lsf_cheatsheet.html#submission-script-template",
    "href": "materials/appendices/lsf_cheatsheet.html#submission-script-template",
    "title": "Appendix A — LSF Quick Reference Guide",
    "section": "A.2 Submission Script Template",
    "text": "A.2 Submission Script Template\nAt the top of the submission shell script, you should have your #BSUB options. Use this as a general template for your scripts:\n#!/bin/bash\n#BSUB -J my_simulation                        # a job name for convenience\n#BSUB -cwd /home/xyz123/scratch/simulations   # your working directory\n#BSUB -o job_logs/simulation.out    # standard output (and standard error if omitting -e) will be saved in this file\n#BSUB -q normal                 # partition\n#BSUB -n2                       # number of CPUs\n#BSUB -R\"select[mem&gt;1000] rusage[mem=1000]\"   # RAM memory part 1\n#BSUB -M1000                                  # RAM memory part 2 \n#BSUB -W15                          # Time for the job in (hh:)mm",
    "crumbs": [
      "Slides",
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>LSF Quick Reference Guide</span>"
    ]
  }
]